{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0991bbd0",
   "metadata": {},
   "source": [
    "# Embodied Agent Interface Challenge @ NeurIPS 2025\n",
    "\n",
    "Welcome to the **Embodied Agent Interface (EAI) Challenge**, a NeurIPS 2025 competition that introduces a unified benchmarking framework for evaluating **Large Language Models (LLMs)** in **embodied decision-making tasks**. This competition aims to foster reproducible research and rigorous analysis in embodied AI, bridging the gap between language modeling and robotic planning.\n",
    "\n",
    "In this tutorial, we will guide you step-by-step through preparing your submission for the EAI Challenge. To keep participation accessible to the broader embodied AI community, setting up complex simulators or environments is optional, though you are more than welcome to do so if you would like to fine-tune your model for better performance. All you need to do is simply follow the steps outlined here and submit your output files in the required format. Enjoy the journey and have fun!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02698c64",
   "metadata": {},
   "source": [
    "## Resources\n",
    "To help you get up to speed and make the most of the **EAI Challenge**, we have prepared a set of essential resources. Feel free to explore them in the following order for the smoothest experience:\n",
    "\n",
    "- **üìÑ Paper**: [Understanding the EAI Challenge and its Objectives](https://arxiv.org/abs/2410.07166)\n",
    "\n",
    "- **üìù Tutorial**: [Step-by-step guide to setting up your environment and understanding the challenge](https://github.com/embodied-agent-interface/embodied-agent-interface)\n",
    "\n",
    "- **üìñ Documentation**: [Complete reference for evaluating and troubleshooting four ability modules](https://embodied-agent-eval.readthedocs.io/)\n",
    "\n",
    "- **üê≥ Docker Image**: [Prebuilt environment for running your experiments hassle-free](https://hub.docker.com/r/jameskrw/eai-eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcbb1925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import torch\n",
    "import openai\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1ccfc",
   "metadata": {},
   "source": [
    "## Prompt Structure\n",
    "All the necessary prompt files are located in the `llm_prompts` directory. There are 8 prompt files in total, each containing a set of prompts for evaluating a specific ability module within a given simulation environment.\n",
    "Before we dive into the implementation details, it's crucial to understand the structure of the prompts you'll be working with. Each prompt is expected to be a JSON object containing the following fields:\n",
    "\n",
    "- `identifier`: A unique task identifier for the prompt.\n",
    "- `llm_prompt`: The actual text prompt to be fed into the language model.\n",
    "\n",
    "Here's an example of what a prompt might look like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"identifier\": \"bringing_in_wood_0_Benevolence_1_int_0_2021-09-15_18-42-25\",\n",
    "    \"llm_prompt\": \"Problem: You are designing instructions for a household robot. The goal is to guide the robot to modify its environment from ...\"\n",
    "}\n",
    "```\n",
    "\n",
    "Familiarizing yourself with this structure will help you navigate the prompt files more effectively and ensure that your submissions are correctly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090f8db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llm_prompts/behavior_action_sequencing_prompts.json',\n",
       " 'llm_prompts/behavior_goal_interpretation_prompts.json',\n",
       " 'llm_prompts/behavior_subgoal_decomposition_prompts.json',\n",
       " 'llm_prompts/behavior_transition_modeling_prompts.json',\n",
       " 'llm_prompts/virtualhome_action_sequencing_prompts.json',\n",
       " 'llm_prompts/virtualhome_goal_interpretation_prompts.json',\n",
       " 'llm_prompts/virtualhome_subgoal_decomposition_prompts.json',\n",
       " 'llm_prompts/virtualhome_transition_modeling_prompts.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_dir = \"llm_prompts\"\n",
    "prompt_files =[os.path.join(prompt_dir, file) for file in os.listdir(prompt_dir) if file.endswith(\".json\")]\n",
    "prompt_files = sorted(prompt_files)\n",
    "prompt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b7f0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behavior_action_sequencing_prompts.json has 100 prompts.\n",
      "behavior_goal_interpretation_prompts.json has 100 prompts.\n",
      "behavior_subgoal_decomposition_prompts.json has 100 prompts.\n",
      "behavior_transition_modeling_prompts.json has 100 prompts.\n",
      "virtualhome_action_sequencing_prompts.json has 342 prompts.\n",
      "virtualhome_goal_interpretation_prompts.json has 342 prompts.\n",
      "virtualhome_subgoal_decomposition_prompts.json has 338 prompts.\n",
      "virtualhome_transition_modeling_prompts.json has 296 prompts.\n"
     ]
    }
   ],
   "source": [
    "for prompt_file in prompt_files:\n",
    "    with open(prompt_file, \"r\") as f:\n",
    "        prompts = json.load(f)\n",
    "    print(f\"{os.path.basename(prompt_file)} has {len(prompts)} prompts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b90331",
   "metadata": {},
   "source": [
    "## Submission Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62f8e1",
   "metadata": {},
   "source": [
    "### Approach 1: Using Hugging Face Transformers\n",
    "If you prefer to use the Hugging Face `transformers` library as evaluation pipelines, here are the key steps to prepare your submission:\n",
    "\n",
    "1. **Install Dependencies**: Make sure you have the necessary libraries installed. You can do this by running:\n",
    "   ```bash\n",
    "   pip install transformers\n",
    "   ```\n",
    "\n",
    "2. **Load Model and Tokenizer**: Use the following code to load the model and tokenizer:\n",
    "   ```python\n",
    "   from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "   model = AutoModelForCausalLM.from_pretrained(\"your_model_name\")\n",
    "   tokenizer = AutoTokenizer.from_pretrained(\"your_model_name\")\n",
    "   ```\n",
    "\n",
    "3. **Generate Outputs**: Use the model to generate outputs for each prompt. Make sure to save the outputs in the specified format.\n",
    "\n",
    "   - `identifier`: A unique task identifier for the prompt.\n",
    "   - `llm_output`: The actual text output generated by the language model.\n",
    "\n",
    "   Here is an example of the expected output format for a corresponding input prompt:\n",
    "\n",
    "   ```json\n",
    "   {\n",
    "      \"identifier\": \"bringing_in_wood_0_Benevolence_1_int_0_2021-09-15_18-42-25\",\n",
    "      \"llm_output\": \"[{\\\"action\\\":\\\"RIGHT_GRASP\\\",\\\"object\\\":\\\"plywood_0\\\"},{\\\"action\\\":\\\"RIGHT_PLACE_ONTOP\\\",\\\"object\\\":\\\"room_floor_kitchen_0\\\"},       {\\\"action\\\":\\\"LEFT_GRASP\\\",\\\"object\\\":\\\"plywood_1\\\"},{\\\"action\\\":\\\"LEFT_PLACE_ONTOP\\\",\\\"object\\\":\\\"room_floor_kitchen_0\\\"},{\\\"action\\\":\\\"LEFT_GRASP\\\",\\\"object\\\":\\\"plywood_2\\\"},{\\\"action\\\":\\\"LEFT_PLACE_ONTOP\\\",\\\"object\\\":\\\"room_floor_kitchen_0\\\"}]\"\n",
    "   }\n",
    "   ```\n",
    "\n",
    "\n",
    "4. **Organize Output Files**: Place all generated output files in a dedicated directory, such as `sample_submission`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ffb68b",
   "metadata": {},
   "source": [
    "Below we use Qwen3-0.6B as an example to demonstrate this process. More details can be found in the [official documentation](https://huggingface.co/Qwen/Qwen3-0.6B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71accc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-14B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d75e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwen_gen(model, tokenizer, prompt):\n",
    "    # prepare the model input\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False # switches between thinking and non-thinking modes. Default is True.\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # conduct text completion\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=40960\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "\n",
    "    # thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d48884",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt_file in tqdm(prompt_files, desc=\"Prompt files\", leave=True):\n",
    "    with open(prompt_file, \"r\") as f:\n",
    "        prompts = json.load(f)\n",
    "\n",
    "    responses = []\n",
    "    for prompt in tqdm(prompts, desc=f\"Processing {os.path.basename(prompt_file)}\", leave=True):\n",
    "        identifier = prompt['identifier']\n",
    "        prompt_text = prompt['llm_prompt']\n",
    "        llm_output = qwen_gen(model, tokenizer, prompt_text)\n",
    "        responses.append({\n",
    "            \"identifier\": identifier,\n",
    "            \"llm_output\": llm_output\n",
    "        })\n",
    "\n",
    "    outputs_save_path = os.path.join(\"sample_submission\", f\"{os.path.basename(prompt_file).split('_prompts')[0]}_outputs.json\")\n",
    "    os.makedirs(os.path.dirname(outputs_save_path), exist_ok=True)\n",
    "    with open(outputs_save_path, \"w\") as f:\n",
    "        json.dump(responses, f, indent=4)\n",
    "# clear the GPU memory\n",
    "del tokenizer\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e69d5",
   "metadata": {},
   "source": [
    "## Approach 2: Access Models through API\n",
    "If you prefer to use proprietary models through an API, here are the key steps to prepare your submission:\n",
    "\n",
    "1. **Choose a Model**: Select a proprietary model that suits your needs. Make sure to review the documentation for any specific requirements or limitations.\n",
    "\n",
    "2. **Set Up API Access**: Follow the provider's instructions to set up API access. This may involve creating an account, obtaining API keys, and installing any necessary libraries.\n",
    "\n",
    "3. **Generate Outputs**: Use the model to generate outputs for each prompt. Make sure to save the outputs in the specified format.\n",
    "\n",
    "   - `identifier`: A unique task identifier for the prompt.\n",
    "   - `llm_output`: The actual text output generated by the language model.\n",
    "\n",
    "   Here is an example of the expected output format for a corresponding input prompt:\n",
    "\n",
    "   ```json\n",
    "   {\n",
    "      \"identifier\": \"bringing_in_wood_0_Benevolence_1_int_0_2021-09-15_18-42-25\",\n",
    "      \"llm_output\": \"[{\\\"action\\\":\\\"RIGHT_GRASP\\\",\\\"object\\\":\\\"plywood_0\\\"},{\\\"action\\\":\\\"RIGHT_PLACE_ONTOP\\\",\\\"object\\\":\\\"room_floor_kitchen_0\\\"},       {\\\"action\\\":\\\"LEFT_GRASP\\\",\\\"object\\\":\\\"plywood_1\\\"},{\\\"action\\\":\\\"LEFT_PLACE_ONTOP\\\",\\\"object\\\":\\\"room_floor_kitchen_0\\\"},{\\\"action\\\":\\\"LEFT_GRASP\\\",\\\"object\\\":\\\"plywood_2\\\"},{\\\"action\\\":\\\"LEFT_PLACE_ONTOP\\\",\\\"object\\\":\\\"room_floor_kitchen_0\\\"}]\"\n",
    "   }\n",
    "   ```\n",
    "\n",
    "4. **Organize Output Files**: Place all generated output files in a dedicated directory, such as `sample_submission`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use OpenAI-compatible API as an example\n",
    "# Depending on your LLM provider, you might need to adjust the client initialization and request format to support concurrent requests\n",
    "def model_gen(model_name, prompt, max_retries=88):\n",
    "    client = OpenAI(\n",
    "        base_url='<Put Your API URL Here>', # e.g. https://openrouter.ai/api/v1\n",
    "        api_key='<Put Your API KEY Here>'\n",
    "    )\n",
    "    current_attempt = 0\n",
    "    delay = 1\n",
    "    while current_attempt < max_retries:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[\n",
    "                    {\n",
    "                        'role': 'user',\n",
    "                        'content': prompt\n",
    "                    }\n",
    "                ],\n",
    "                stream=False\n",
    "            )\n",
    "            # return response.choices[0].message.reasoning_content, response.choices[0].message.content\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred on attempt {current_attempt + 1}: {e}\")\n",
    "            current_attempt += 1\n",
    "            if current_attempt < max_retries:\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "            else:\n",
    "                print(f\"All {max_retries} retry attempts failed for model {model_name}.\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b0d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/llama-4-maverick\"  # Replace with your desired model name\n",
    "for prompt_file in tqdm(prompt_files, desc=\"Prompt files\", leave=True):\n",
    "    with open(prompt_file, \"r\") as f:\n",
    "        prompts = json.load(f)\n",
    "\n",
    "    responses = []\n",
    "    for prompt in tqdm(prompts, desc=f\"Processing {os.path.basename(prompt_file)}\", leave=True):\n",
    "        identifier = prompt['identifier']\n",
    "        prompt_text = prompt['llm_prompt']\n",
    "        # reasoning, answer = model_gen(model_name, prompt_text)\n",
    "        answer = model_gen(model_name, prompt_text)\n",
    "        responses.append({\n",
    "            \"identifier\": identifier,\n",
    "            \"llm_output\": answer\n",
    "        })\n",
    "\n",
    "    outputs_save_path = os.path.join(\"sample_submission\", f\"{os.path.basename(prompt_file).split('_prompts')[0]}_outputs.json\")\n",
    "    os.makedirs(os.path.dirname(outputs_save_path), exist_ok=True)\n",
    "    with open(outputs_save_path, \"w\") as f:\n",
    "        json.dump(responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131db8b9",
   "metadata": {},
   "source": [
    "## Submission on EvalAI\n",
    "Before you submit your outputs, ensure that they are properly organized and meet the submission requirements. Here are some key steps to follow:\n",
    "\n",
    "1. **Organize Output Files**: Place all generated output files in a dedicated directory, such as `sample_submission`.\n",
    "2. **Naming Convention**: Ensure that each output file is named according to the corresponding prompt file, with `_outputs` appended before the file extension.\n",
    "3. **Review File Structure**: Double-check the directory structure to make sure it matches the expected format for submission.\n",
    "4. **Final Review**: Conduct a final review of all output files to ensure they are correctly formatted and contain the expected data.\n",
    "5. **Zip the Directory**: Compress the `sample_submission` directory into a zip file for submission.\n",
    "\n",
    "An example submission structure should look like this:\n",
    "\n",
    "```\n",
    "sample_submission.zip/\n",
    "‚îú‚îÄ‚îÄ behavior_action_sequencing_outputs.json\n",
    "‚îú‚îÄ‚îÄ behavior_goal_interpretation_outputs.json\n",
    "‚îú‚îÄ‚îÄ behavior_subgoal_decomposition_outputs.json\n",
    "‚îú‚îÄ‚îÄ behavior_transition_modeling_outputs.json\n",
    "‚îú‚îÄ‚îÄ virtualhome_action_sequencing_outputs.json\n",
    "‚îú‚îÄ‚îÄ virtualhome_goal_interpretation_outputs.json\n",
    "‚îú‚îÄ‚îÄ virtualhome_subgoal_decomposition_outputs.json\n",
    "‚îî‚îÄ‚îÄ virtualhome_transition_modeling_outputs.json\n",
    "```\n",
    "\n",
    "After making sure the submission structure is correct, you can proceed with the following steps detailed in the [Participate page](https://neurips25-eai.github.io/participate) to complete your submission on EvalAI and track your performance on the [Leaderboard](https://eval.ai/web/challenges/challenge-page/2621/leaderboard).\n",
    "\n",
    "Note:\n",
    "The 8 output files in sample_submission folder is generated by the Qwen3-14B model. It will not give you a super high score, but it can serve as a useful reference for your own submissions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
